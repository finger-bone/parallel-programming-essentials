"use strict";(self.webpackChunknotes_template=self.webpackChunknotes_template||[]).push([[4358],{1200:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>p,frontMatter:()=>i,metadata:()=>l,toc:()=>d});const l=JSON.parse('{"id":"parallel-patterns","title":"Parallel Patterns","description":"Parallel patterns are fundamental building blocks in parallel programming, similar to how design patterns work in object-oriented programming. These patterns provide proven solutions to common parallel programming challenges.","source":"@site/docs/parallel-patterns.mdx","sourceDirName":".","slug":"/parallel-patterns","permalink":"/parallel-programming-essentials/docs/parallel-patterns","draft":false,"unlisted":false,"editUrl":"https://github.com/finger-bone/parallel-programming-essentials/blob/main/docs/parallel-patterns.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","next":{"title":"SYCL Quickstart","permalink":"/parallel-programming-essentials/docs/category/sycl-quickstart"}}');var s=a(4848),t=a(8453);const i={sidebar_position:2},r="Parallel Patterns",c={},d=[{value:"Data Parallelism",id:"data-parallelism",level:2},{value:"Concept",id:"concept",level:3},{value:"Example Implementation",id:"example-implementation",level:3},{value:"Task Parallelism",id:"task-parallelism",level:2},{value:"Concept",id:"concept-1",level:3},{value:"Example Implementation",id:"example-implementation-1",level:3},{value:"Map and Reduce",id:"map-and-reduce",level:2},{value:"Map Pattern",id:"map-pattern",level:3},{value:"Reduce Pattern",id:"reduce-pattern",level:3}];function o(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"parallel-patterns",children:"Parallel Patterns"})}),"\n",(0,s.jsx)(n.p,{children:"Parallel patterns are fundamental building blocks in parallel programming, similar to how design patterns work in object-oriented programming. These patterns provide proven solutions to common parallel programming challenges."}),"\n",(0,s.jsx)(n.h2,{id:"data-parallelism",children:"Data Parallelism"}),"\n",(0,s.jsx)(n.h3,{id:"concept",children:"Concept"}),"\n",(0,s.jsx)(n.p,{children:"Data parallelism is a pattern where the same operation is performed on multiple data elements simultaneously. Think of it like having multiple workers processing different parts of a large dataset at the same time. This pattern works best when:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"The data can be split into independent chunks"}),"\n",(0,s.jsx)(n.li,{children:"The same operation needs to be applied to each element"}),"\n",(0,s.jsx)(n.li,{children:"There are no dependencies between calculations"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"example-implementation",children:"Example Implementation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-cpp",children:"#include <sycl/sycl.hpp>\n#include <iostream>\n\nint main() {\n    const size_t N = 1024;\n    sycl::queue q;\n\n    // Initialize input vectors\n    std::vector<float> a(N, 1.0f);\n    std::vector<float> b(N, 2.0f);\n    std::vector<float> c(N);\n\n    // Create SYCL buffers\n    sycl::buffer<float> buf_a(a.data(), N);\n    sycl::buffer<float> buf_b(b.data(), N);\n    sycl::buffer<float> buf_c(c.data(), N);\n\n    // Perform parallel vector addition\n    q.submit([&](sycl::handler& h) {\n        auto acc_a = buf_a.get_access<sycl::access::mode::read>(h);\n        auto acc_b = buf_b.get_access<sycl::access::mode::read>(h);\n        auto acc_c = buf_c.get_access<sycl::access::mode::write>(h);\n        \n        h.parallel_for(sycl::range<1>{N}, [=](sycl::id<1> idx) {\n            acc_c[idx] = acc_a[idx] + acc_b[idx];  // Each element processed in parallel\n        });\n    });\n\n    q.wait();\n    return 0;\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"task-parallelism",children:"Task Parallelism"}),"\n",(0,s.jsx)(n.h3,{id:"concept-1",children:"Concept"}),"\n",(0,s.jsx)(n.p,{children:"Task parallelism focuses on running different functions or tasks simultaneously. Unlike data parallelism, which splits data, task parallelism splits the work itself. It's useful when:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"You have multiple independent operations to perform"}),"\n",(0,s.jsx)(n.li,{children:"Tasks might be different from each other"}),"\n",(0,s.jsx)(n.li,{children:"Tasks can run independently"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"example-implementation-1",children:"Example Implementation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-cpp",children:"#include <sycl/sycl.hpp>\n\nint main() {\n    sycl::queue q1, q2;  // Two separate queues for concurrent tasks\n    const size_t N = 1024;\n    std::vector<int> data1(N), data2(N);\n\n    // Task 1: Double each element\n    q1.submit([&](sycl::handler& cgh) {\n        sycl::accessor acc(data1, cgh, sycl::write_only);\n        cgh.parallel_for(sycl::range<1>(N), [=](sycl::id<1> i) {\n            acc[i] = i[0] * 2;\n        });\n    });\n\n    // Task 2: Calculate modulo 100\n    q2.submit([&](sycl::handler& cgh) {\n        sycl::accessor acc(data2, cgh, sycl::write_only);\n        cgh.parallel_for(sycl::range<1>(N), [=](sycl::id<1> i) {\n            acc[i] = i[0] % 100;\n        });\n    });\n\n    q1.wait();\n    q2.wait();\n    return 0;\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"map-and-reduce",children:"Map and Reduce"}),"\n",(0,s.jsx)(n.p,{children:"These are two fundamental parallel patterns often used together:"}),"\n",(0,s.jsx)(n.h3,{id:"map-pattern",children:"Map Pattern"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Transforms each element in a dataset independently"}),"\n",(0,s.jsx)(n.li,{children:"Perfect for parallel execution since each operation is independent"}),"\n",(0,s.jsx)(n.li,{children:"Example: squaring each number in an array"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-cpp",children:"#include <sycl/sycl.hpp>\n\nint main() {\n    sycl::queue q;\n    const size_t N = 1024;\n    std::vector<int> data(N);\n\n    // Map operation: square each element\n    q.submit([&](sycl::handler& cgh) {\n        cgh.parallel_for(sycl::range<1>(N), [=](sycl::id<1> i) {\n            data[i] = i[0] * i[0];\n        });\n    }).wait();\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"reduce-pattern",children:"Reduce Pattern"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Combines all elements in a dataset into a single result"}),"\n",(0,s.jsx)(n.li,{children:"Uses divide-and-conquer to parallelize the reduction"}),"\n",(0,s.jsx)(n.li,{children:"Example: summing all numbers in an array"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-cpp",children:"#include <sycl/sycl.hpp>\n\nint main() {\n    sycl::queue q;\n    const size_t N = 1024;\n    std::vector<int> data(N, 1);  // Initialize with 1s\n    sycl::buffer<int> buf(data.data(), sycl::range<1>(N));\n\n    // Parallel reduction\n    size_t size = N;\n    while (size > 1) {\n        size_t new_size = (size + 1) / 2;\n        q.submit([&](sycl::handler& h) {\n            auto acc = buf.get_access<sycl::access::mode::read_write>(h);\n            \n            h.parallel_for(sycl::range<1>(new_size), [=](sycl::id<1> idx) {\n                size_t i = idx[0];\n                size_t pair_idx = i + new_size;\n                if (pair_idx < size) {\n                    acc[i] += acc[pair_idx];  // Combine pairs of elements\n                }\n            });\n        });\n        size = new_size;\n        q.wait();\n    }\n}\n"})})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(o,{...e})}):o(e)}}}]);